{"meta":{"title":"Go for it! Just do it!","subtitle":"","description":"zmq的个人博客","author":"zmq","url":"https://zmq-git.github.io","root":"/zmq-git.github.io/"},"pages":[{"title":"category","date":"2021-03-20T05:32:08.000Z","updated":"2021-03-20T07:08:41.960Z","comments":true,"path":"category/index.html","permalink":"https://zmq-git.github.io/category/index.html","excerpt":"","text":""},{"title":"about","date":"2021-03-23T02:31:03.000Z","updated":"2022-03-13T01:49:31.916Z","comments":true,"path":"about/index.html","permalink":"https://zmq-git.github.io/about/index.html","excerpt":"","text":"​ - 事事有回应，件件有着落 -"},{"title":"tag","date":"2021-03-20T05:32:08.000Z","updated":"2021-03-20T07:08:28.378Z","comments":true,"path":"tag/index.html","permalink":"https://zmq-git.github.io/tag/index.html","excerpt":"","text":""}],"posts":[{"title":"类加载机制和类加载器","slug":"类加载机制和类加载器","date":"2022-04-19T07:00:11.000Z","updated":"2022-04-19T07:34:34.246Z","comments":true,"path":"2022/04/19/类加载机制和类加载器/","link":"","permalink":"https://zmq-git.github.io/2022/04/19/%E7%B1%BB%E5%8A%A0%E8%BD%BD%E6%9C%BA%E5%88%B6%E5%92%8C%E7%B1%BB%E5%8A%A0%E8%BD%BD%E5%99%A8/","excerpt":"","text":"类加载机制Java 程序从源代码到运⾏⼀般有下⾯ 3 步： ​ 我们首先从“.java”代码文件，编译成“.class”字节码文件 然后类加载器把“.class”字节码文件中的类给加载到JVM中 ，接着是JVM来执行我们写好的那些类中的代码，整体是这么个顺序。 一个类从加载到使用，一般会经历下面的以下过程： 加载：当我们的代码中用到这个类的时候就会去加载这个类（.class）到JVM中 验证：根据Java虚拟机规范，来校验你加载进来的“.class”文件中的内容，是否符合指定的规范 准备：给这个类分配内存，给静态变量分配内存并且赋予初始值 解析：把符号引用替换为直接引用 初始化：显示初始化静态变量，调用静态代码块 类加载器启动类加载器​ Bootstrap ClassLoader，他主要是负责加载我们在机器上安装的Java目录下的核心类的 相信大家都知道，如果你要在一个机器上运行自己写好的Java系统，无论是windows笔记本，还是linux服务器，是不是都得装一下 JDK？ 那么在你的Java安装目录下，就有一个“lib”目录，大家可以自己去找找看，这里就有Java最核心的一些类库，支撑你的Java系统的 运行。 所以一旦你的JVM启动，那么首先就会依托启动类加载器，去加载你的Java安装目录下的“lib”目录中的核心类库。 扩展类加载器​ Extension ClassLoader，这个类加载器其实也是类似的，就是你的Java安装目录下，有一个“lib\\ext”目录 这里面有一些类，就是需要使用这个类加载器来加载的，支撑你的系统的运行。 应用程序类加载器​ Application ClassLoader，这类加载器就负责去加载“ClassPath”环境变量所指定的路径中的类 其实你大致就理解为去加载你写好的Java代码吧，这个类加载器就负责加载你写好的那些类到内存里。 自定义类加载器​ 除了上面那几种之外，还可以自定义类加载器，去根据你自己的需求加载你的类。 双亲委派机制​ JVM的类加载器是有亲子层级结构的，就是说启动类加载器是最上层的，扩展类加载器在第二层，第三层是应用程序类加载器，最后一 层是自定义类加载器。 如果应用程序类加载器需要加载一个类，并不会立马自己去加载，而是会去委托扩展类加载器加载，而扩展类加载器会委托启动类加载器去加载，当启动类加载器找不到这个类时，会让扩展类加载器去找，扩展类加载器找不到，于是就轮到应用程序类加载器自己去找了。这样的话，可以避免多层级的加载器结构重复加载某些类。","categories":[{"name":"JVM","slug":"JVM","permalink":"https://zmq-git.github.io/categories/JVM/"}],"tags":[{"name":"学习笔记","slug":"学习笔记","permalink":"https://zmq-git.github.io/tags/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"}]},{"title":"MySQL之索引","slug":"MySQL之索引","date":"2022-03-10T08:44:54.000Z","updated":"2022-03-13T06:22:38.486Z","comments":true,"path":"2022/03/10/MySQL之索引/","link":"","permalink":"https://zmq-git.github.io/2022/03/10/MySQL%E4%B9%8B%E7%B4%A2%E5%BC%95/","excerpt":"","text":"索引（在MySQL中也叫做“键（key）”）是存储引擎用于快速找到记录的一种数据结构。如果将一张表比作一本书，那么索引就是这本书的目录。 索引类型B+树索引B+Tree是在B-Tree基础上的一种优化，使其更适合实现外存储索引结构，InnoDB存储引擎就是用B+Tree实现其索引结构。 B-Tree每个节点中不仅包含数据的key值，还有data值。而每一个页的存储空间是有限的，如果data数据较大时将会导致每个节点（即一个页）能存储的key的数量很小，当存储的数据量很大时同样会导致B-Tree的深度较大，增大查询时的磁盘I/O次数，进而影响查询效率。在B+Tree中，所有数据记录节点都是按照键值大小顺序存放在同一层的叶子节点上，而非叶子节点上只存储key值信息，这样可以大大加大每个节点存储的key值数量，降低B+Tree的高度。 B+Tree相对于B-Tree有几点不同： 非叶子节点只存储键值信息。 所有叶子节点之间都有一个链指针。 数据记录都存放在叶子节点中。 B+Tree结构如下图所示（聚簇索引）： ​ 当使用索引检索时，会从索引的根节点开始搜索，根节点的槽中存放了指向子节点的指针，存储引擎根据这些指针向下层查找，通过比较节点页中的值和要查找的值可以找到合适的指针进入下层子节点。这些指针实际上定义了子节点也页中值的上限和下限。最终存储引擎要么是找到对应的值，要么该记录不存在。叶子节点比较特别，它们的指针指向的是被索引的数据，而不是其他的节点页（聚簇索引中叶子节点中保存的是实际的诗句，而非聚簇索引中叶子节点保存的是行的主键值，根据主键再去聚簇索引中找到真实的数据）。 B+树索引的限制： 如果不是按索引的最左列开始查找，则无法使用索引 不能跳过索引中的列 如果查询中有某个列的范围查询，则其右边的所有列都无法使用索引优化查找 哈希索引​ 哈希索引基于哈希表实现，只有精确匹配索引所有列的查询才有效，对于每一行数据，存储引擎都会对所有的索引列计算一个哈希码，哈希码是一个较小的值，并且不同键值计算出来的哈希码也不一样，哈希索引将所有的哈希码存储在索引中，同时在哈希表中保存指向每个数据行的指针。 哈希索引的数据结构如下： 槽（Slot） 值（Value） 2323 指向第一行的指针 2458 指向第四行的指针 7437 指向第二行的指针 8784 指向第三行的指针 每个槽的编号是顺序的，但是数据行不是。如果使用到哈希索引，MySQL会先计算出查询条件的哈希值，并使用该值寻找对应的记录指针。 创建正确的索引独立的列“独立的列”是指索引列能是表达式的一部分，也不能是函数的参数。 例如： 1select actor_id from actor where actor_id + 1= 5; 或是： 1select ... where TO_DAYS(CURRENT_DATE) - TO_DAYS(CREATE_DATE) &lt;= 10; 前缀索引和自定义哈希索引 有时候需要索引很长的字符列，这会让索引变得大而且慢，这时候有两种策略：自定义哈希索引或是选择前缀索引 自定义哈希索引原理还是用B树索引进行查找，只不过使用哈希值而不是键值本身进行查找，需要做的就是在查询的where字句中手动指定使用哈希函数。 示例：例如需要存储大量的URL，并需要根据URL进行搜索，如果使用B+树来存储URL，那么索引会很大，正常情况会有如下查询： 1select id from test where url = &#x27;https://www.baidu.com&#x27;; 现在我们自定义一个哈希索引，新建一个被索引的 ‘hx_url’ 列，使用CRC32做哈希，就可以用下面的方式查询： 1select id from test where url = &#x27;https://www.baidu.com&#x27; and hx_url = CRC32(&#x27;https://www.baidu.com&#x27;); 这样实现的一个缺陷是需要维护哈希值，可以手动维护也可以使用触发器： 1create TRIGGER insert_hx_url before insert on test for each row set NEW.hx_url = CRC32(NEW.hx_url); 另外不同值计算出来的哈希值有可能重复，造成哈希冲突，所以我们使用哈希索引进行查询的时候，必须在where字句中包含我们要查询的常量值。 前缀索引前缀索引是指索引开始的部分字符，这样可以大大节省索引空间，从而提高检索效率。但是这样有可能会降低索引的选择性，索引选择性是指，不重复的索引值和数据表的记录总数（T）的比值，范围从1/T到1之间，索引的选择性越高，MySQL在查询时可以过滤掉更多的行，提高查询效率，如： 有五个水果，如果这五个水果分别是五种不同种类的水果，那么假设我想吃苹果就只需要筛选一次就可以拿到我想要的水果，但是如果五个水果中有三个苹果，那我需要进一步筛选，如大小，长相之类,选择性分别是5/5=1以及3/5=0.6。所以选择性越高那么能过滤掉的数据行也越多 选择性的计算公式： count(distinct city)/count(*)，city为需要建立索引的列。 那么我们要建立前缀索引就需要选择尽量贴近依据这个公式计算出来的结果，如果算出来长度为7时选择性最合适，那么使用以下sql来建立前缀索引： 1alter table demo add key (city(7))","categories":[{"name":"Mysql","slug":"Mysql","permalink":"https://zmq-git.github.io/categories/Mysql/"}],"tags":[{"name":"学习笔记","slug":"学习笔记","permalink":"https://zmq-git.github.io/tags/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"}]},{"title":"MapperScan注解原理分析","slug":"MapperScan注解原理分析","date":"2022-02-17T09:15:29.000Z","updated":"2022-02-27T06:31:18.527Z","comments":true,"path":"2022/02/17/MapperScan注解原理分析/","link":"","permalink":"https://zmq-git.github.io/2022/02/17/MapperScan%E6%B3%A8%E8%A7%A3%E5%8E%9F%E7%90%86%E5%88%86%E6%9E%90/","excerpt":"","text":"@MapperScan注解的作用​ 在Mybatis整合Spring过程中有一个很重要的类就是MapperFactoryBean,这个可以将Mybatis生成的代理类对象添加到Spring容器当中，但是单个类的添加显然不符合Mybatis的要求，所以Mybatis还提供了@MapperScan注解。在SpringBoot+Mybatis的项目中，我们常常会在启动类上添加@MapperScan注解，这个注解可以将声明包下所有的mapper的代理对象添加到spring容器中，所以在mapper接口上，我们没有必要添加@Repository手动注册到spring容器中。 @MapperScan原理简单分析​ 来看看@MapperScan注解： 我们可以发现这个注解上面有个@Import注解，@Import注解是用来导入配置类或者一些需要前置加载的类，@Import支持 三种方式： 带有@Configuration的配置类(4.2 版本之前只可以导入配置类，4.2版本之后 也可以导入 普通类) ImportSelector 的实现 ImportBeanDefinitionRegistrar 的实现。 相当于将MapperScannerRegistrar这个类注册到了Spring容器中；Spring会去执行这个类的registerBeanDefinitions方法： 调用registerBeanDefinitions方法首先判断@MapperScan有没有写参数 条件成立 调用registerBeanDefinitions方法： 这里会生成一个MapperScannerConfigurer类的BeanDefinition，Spring容器最后会去执行这个类中的postProcessBeanDefinitionRegistry方法： 这里会创建一个ClassPathMapperScanner对象，并且去扫描在@MapperScan注解中声明包下的类，进入到ClassPathMapperScanner类中的doScan方法中： 在这里调用父类的doScan方法扫描包下的所有类，然后调用processBeanDefinitions方法： 从这里可以看出，使用@Mapperscan扫描整个dao层的原理还是基于Mybatis给出的MapperFactoryBean类，遍历整个dao层中的每个类对应的BeanDefinition，在第一步调用构造方法给MapperFactoryBean中的mapperInterface赋值，声明要代理的对象，第二步设置BeanDefinition为MapperFactoryBean类，第三步给MapperFactoryBean中的sqlSessionFactory赋值，到此，就把dao下面的类的代理对象注册到了Spring容器中。 可以查看Mybatis-Spring的官网，注册单个Bean的方法： 所以@MapperScan的原理就是根据声明包下的类生成对应的MapperFactoryBean对象。 模拟@MapperScan注解自定义一个@MyMapperScan注解123456@Retention(RetentionPolicy.RUNTIME)@Import(MyImportBeanDefinitionRegister.class)public @interface MyMapperScan &#123; String value() ;&#125; 编写MyImportBeanDefinitionRegister类123456789101112131415161718192021222324252627/** * 自己注册BeanDefinition */public class MyImportBeanDefinitionRegister implements ImportBeanDefinitionRegistrar &#123; @Override public void registerBeanDefinitions(AnnotationMetadata importingClassMetadata, BeanDefinitionRegistry registry) &#123; AnnotationAttributes mapperScanAttrs = AnnotationAttributes .fromMap(importingClassMetadata.getAnnotationAttributes(MyMapperScan.class.getName())); String packageName = mapperScanAttrs.getString(&quot;value&quot;); //根据包名获取包下的Class集合 Set&lt;Class&lt;?&gt;&gt; classSet = ClassUtil.getClassSet(packageName); for(Class c : classSet)&#123; BeanDefinitionBuilder builder = BeanDefinitionBuilder.genericBeanDefinition(MapperFactoryBean.class); AbstractBeanDefinition beanDefinition = builder.getBeanDefinition(); beanDefinition.getPropertyValues().add(&quot;mapperInterface&quot;, c); beanDefinition.getPropertyValues().add(&quot;sqlSessionFactory&quot;, registry.getBeanDefinition(&quot;sqlSessionFactory&quot;)); registry.registerBeanDefinition(generateBeanName(c.getSimpleName()),beanDefinition); &#125; &#125; /** * 生成bean对象的名称 */ public String generateBeanName(String beanName)&#123; return beanName.substring(0,1).toLowerCase()+beanName.substring(1); &#125;&#125;","categories":[{"name":"mybatis","slug":"mybatis","permalink":"https://zmq-git.github.io/categories/mybatis/"}],"tags":[{"name":"学习笔记","slug":"学习笔记","permalink":"https://zmq-git.github.io/tags/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"}]},{"title":"Docker入门篇","slug":"Docker学习篇","date":"2022-02-09T08:09:54.000Z","updated":"2022-02-27T06:49:42.152Z","comments":true,"path":"2022/02/09/Docker学习篇/","link":"","permalink":"https://zmq-git.github.io/2022/02/09/Docker%E5%AD%A6%E4%B9%A0%E7%AF%87/","excerpt":"","text":"Docker的几个概念镜像镜像就是一个只读的模板，镜像可以用来创建 Docker 容器，一个镜像可以创建多个容器 容器容器是用镜像创建的运行实例，Docker 利用容器独立运行一个或一组应用。它可以被启动、开始、停止、删除，每个容器都是相互隔离的、保证安全的平台。可以把容器看作是一个简易的 Linux 环境和运行在其中的应用程序。容器的定义和镜像几乎一模一样，也是一堆层的统一视角，唯一区别在于容器的最上面那一层是可读可写的 仓库仓库是集中存放镜像文件的场所。仓库和仓库注册服务器是有区别的，仓库注册服务器上往往存放着多个仓库，每个仓库中又包含了多个镜像，每个镜像有不同的标签。仓库分为公开仓库和私有仓库两种形式，最大的公开仓库是 DockerHub，存放了数量庞大的镜像供用户下载，国内的公开仓库有阿里云、网易云等 总结通俗点说，一个镜像就代表一个软件；而基于某个镜像运行就是生成一个程序实例，这个程序实例就是容器；而仓库是用来存储 Docker 中所有镜像的。 其中仓库又分为远程仓库和本地仓库，和 Maven 类似，倘若每次都从远程下载依赖，则会大大降低效率，为此，Maven 的策略是第一次访问依赖时，将其下载到本地仓库，第二次、第三次使用时直接用本地仓库的依赖即可，Docker 的远程仓库和本地仓库的作用也是类似的。 安装Docker 这里以Centos7为例 在测试或开发环境中，Docker 官方为了简化安装流程，提供了一套便捷的安装脚本，执行这个脚本后就会自动地将一切准备工作做好，并且把 Docker 的稳定版本安装在系统中。 12curl -fsSL get.docker.com -o get-docker.shsh get-docker.sh --mirror Aliyun 安装完成后直接启动服务： 1systemctl start docker 推荐设置开机自启，执行指令： 1systemctl enable docker Docker初体验 Docker 提供了一个 DockerHub 用于查询各种镜像的地址和安装教程 DockerHub： https://hub.docker.com/，这里安装一下Mysql镜像 在左上角的搜索框中输入MySQL并回车： 可以看到相关 MySQL 的镜像非常多，若右上角有OFFICIAL IMAGE标识，则说明是官方镜像，所以我们点击第一个 MySQL 镜像： 右边提供了下载 MySQL 镜像的指令为docker pull MySQL，但该指令始终会下载 MySQL 镜像的最新版本。 若是想下载指定版本的镜像，则点击下面的View Available Tags： 这里就可以看到各种版本的镜像，右边有下载的指令，所以若是想下载 5.7.32 版本的 MySQL 镜像，则执行： 1docker pull MySQL:5.7.32 Docker 镜像指令Docker 需要频繁地操作相关的镜像，所以我们先来了解一下 Docker 中的镜像指令。 若想查看 Docker 中当前拥有哪些镜像，则可以使用 docker images 命令。 123456[root@izrcf5u3j3q8xaz ~]# docker imagesREPOSITORY TAG IMAGE ID CREATED SIZEMySQL 5.7.32 f07dfa83b528 11 days ago 448MBtomcat latest feba8d001e3f 2 weeks ago 649MBnginx latest ae2feff98a0c 2 weeks ago 133MBhello-world latest bf756fb1ae65 12 months ago 13.3kB 其中REPOSITORY为镜像名，TAG为版本标志，IMAGE ID为镜像 id(唯一的)，CREATED为创建时间，注意这个时间并不是我们将镜像下载到 Docker 中的时间，而是镜像创建者创建的时间，SIZE为镜像大小。 该指令能够查询指定镜像名： 1docker image MySQL 若如此做，则会查询出 Docker 中的所有 MySQL 镜像： 12345[root@izrcf5u3j3q8xaz ~]# docker images MySQLREPOSITORY TAG IMAGE ID CREATED SIZEMySQL 5.6 0ebb5600241d 11 days ago 302MBMySQL 5.7.32 f07dfa83b528 11 days ago 448MBMySQL 5.5 d404d78aa797 20 months ago 205MB 该指令还能够携带-p参数：docker images -q ， -q表示仅显示镜像的 id： 12345[root@izrcf5u3j3q8xaz ~]# docker images -q0ebb5600241df07dfa83b528feba8d001e3fd404d78aa797 若是要下载镜像，则使用： 1docker pull MySQL:5.7 docker pull是固定的，后面写上需要下载的镜像名及版本标志；若是不写版本标志，而是直接执行docker pull MySQL，则会下载镜像的最新版本。 一般在下载镜像前我们需要搜索一下镜像有哪些版本才能对指定版本进行下载，使用指令： 1docker search MySQL 不过该指令只能查看 MySQL 相关的镜像信息，而不能知道有哪些版本，若想知道版本，则只能这样查询： 1docker search MySQL:5.5 若是查询的版本不存在，则结果为空： 删除镜像使用指令： 1docker image rm MySQL:5.5 若是不指定版本，则默认删除的也是最新版本。 还可以通过指定镜像 id 进行删除： 1docker image rm bf756fb1ae65 然而此时报错了： 12[root@izrcf5u3j3q8xaz ~]# docker image rm bf756fb1ae65Error response from daemon: conflict: unable to delete bf756fb1ae65 (must be forced) - image is being used by stopped container d5b6c177c151 这是因为要删除的hello-world镜像正在运行中，所以无法删除镜像，此时需要强制执行删除： 1docker image rm -f bf756fb1ae65 该指令会将镜像和通过该镜像执行的容器全部删除，谨慎使用。 Docker 容器指令掌握了镜像的相关指令之后，我们需要了解一下容器的指令，容器是基于镜像的。 若需要通过镜像运行一个容器，则使用： 1docker run tomcat:8.0-jre8 当然了，运行的前提是你拥有这个镜像，所以先下载镜像： 1docker pull tomcat:8.0-jre8 下载完成后就可以运行了，运行后查看一下当前运行的容器：docker ps 。 其中CONTAINER_ID为容器的 id，IMAGE为镜像名，COMMAND为容器内执行的命令，CREATED为容器的创建时间，STATUS为容器的状态，PORTS为容器内服务监听的端口，NAMES为容器的名称。 通过该方式运行的 tomcat 是不能直接被外部访问的，因为容器具有隔离性，若是想直接通过 8080 端口访问容器内部的 tomcat，则需要对宿主机端口与容器内的端口进行映射： 1docker run -p 8080:8080 tomcat:8.0-jre8 解释一下这两个端口的作用(8080:8080)，第一个 8080 为宿主机端口，第二个 8080 为容器内的端口，外部访问 8080 端口就会通过映射访问容器内的 8080 端口。 此时外部就可以访问 Tomcat 了： 若是这样进行映射： 1docker run -p 8088:8080 tomcat:8.0-jre8 则外部需访问 8088 端口才能访问 tomcat，需要注意的是，每次运行的容器都是相互独立的，所以同时运行多个 tomcat 容器并不会产生端口的冲突。 容器还能够以后台的方式运行，这样就不会占用终端： 1docker run -d -p 8080:8080 tomcat:8.0-jre8 启动容器时默认会给容器一个名称，但这个名称其实是可以设置的，使用指令： 1docker run -d -p 8080:8080 --name tomcat01 tomcat:8.0-jre8 此时的容器名称即为 tomcat01，容器名称必须是唯一的。 再来引申一下docker ps中的几个指令参数，比如-a： 1docker ps -a 该参数会将运行和非运行的容器全部列举出来： -q参数将只查询正在运行的容器 id：docker ps -q 。 12345[root@izrcf5u3j3q8xaz ~]# docker ps -qf3aac8ee94a3074bf575249b1d557472a7084421848ba294 若是组合使用，则查询运行和非运行的所有容器 id：docker ps -qa 。 1234567891011[root@izrcf5u3j3q8xaz ~]# docker ps -aqf3aac8ee94a37f7b0e80c841074bf575249ba1e830bddc4c1d557472a7084421848ba294b0440c0a219ac2f5d78c5d1a5831d1bab2a6d5b6c177c151 接下来是容器的停止、重启指令，因为非常简单，就不过多介绍了。 1docker start c2f5d78c5d1a 通过该指令能够将已经停止运行的容器运行起来，可以通过容器的 id 启动，也可以通过容器的名称启动。 1docker restart c2f5d78c5d1a 该指令能够重启指定的容器。 1docker stop c2f5d78c5d1a 该指令能够停止指定的容器。 1docker kill c2f5d78c5d1a 该指令能够直接杀死指定的容器。 以上指令都能够通过容器的 id 和容器名称两种方式配合使用。 当容器被停止之后，容器虽然不再运行了，但仍然是存在的，若是想删除它，则使用指令： 1docker rm d5b6c177c151 需要注意的是容器的 id 无需全部写出来，只需唯一标识即可。 若是想删除正在运行的容器，则需要添加-f参数强制删除： 1docker rm -f d5b6c177c151 若是想删除所有容器，则可以使用组合指令： 1docker rm -f $(docker ps -qa) 先通过docker ps -qa查询出所有容器的 id，然后通过docker rm -f进行删除。 当容器以后台的方式运行时，我们无法知晓容器的运行状态，若此时需要查看容器的运行日志，则使用指令： 1docker logs 289cc00dc5ed 这样的方式显示的日志并不是实时的，若是想实时显示，需要使用-f参数： 1docker logs -f 289cc00dc5ed 通过-t参数还能够显示日志的时间戳，通常与-f参数联合使用： 1docker logs -ft 289cc00dc5ed 查看容器内运行了哪些进程，可以使用指令： 1docker top 289cc00dc5ed 若是想与容器进行交互，则使用指令： 1docker exec -it 289cc00dc5ed bash 此时终端将会进入容器内部，执行的指令都将在容器中生效，在容器内只能执行一些比较简单的指令，如：ls、cd 等，若是想退出容器终端，重新回到 CentOS 中，则执行exit即可。","categories":[{"name":"Docker","slug":"Docker","permalink":"https://zmq-git.github.io/categories/Docker/"}],"tags":[{"name":"学习笔记","slug":"学习笔记","permalink":"https://zmq-git.github.io/tags/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"}]},{"title":"搭建FastDFS分布式文件管理系统","slug":"搭建FastDfs分布式文件管理系统","date":"2022-01-05T07:33:54.000Z","updated":"2022-01-25T01:28:04.786Z","comments":true,"path":"2022/01/05/搭建FastDfs分布式文件管理系统/","link":"","permalink":"https://zmq-git.github.io/2022/01/05/%E6%90%AD%E5%BB%BAFastDfs%E5%88%86%E5%B8%83%E5%BC%8F%E6%96%87%E4%BB%B6%E7%AE%A1%E7%90%86%E7%B3%BB%E7%BB%9F/","excerpt":"","text":"最近在做项目时发现文件上传下载用到了FastDFS，想到自己对这个还不熟悉，于是想自己在服务器上搭一下服务，趁机了解一下FastDFS。网上有许多关于FastDFS的搭建教程，但是没想到自己前后折腾了一整天才给弄好，踩了许多坑，呼，这里记录一下。 关于FastDFS作为一款分布式文件管理系统，FastDFS 主要包括四个方面的功能： 文件存储 文件同步 文件上传 文件下载 这个方面的功能，基本上就能搞定我们常见的文件管理需求了。 下面这是一张来自 FastDFS 官网的系统架构图： 从上面这张图中我们可以看到，FastDFS 架构包括 Tracker 和 Storage 两部分，看名字大概就能知道，Tracker 用来追踪文件，相当于是文件的一个索引，而 Storage 则用来保存文件。 我们上传文件的文件最终保存在 Storage 上，文件的元数据信息保存在 Tracker 上，通过 Tracker 可以实现对 Storage 的负载均衡。 Storage 一般会搭建成集群，一个 Storage Cluster 可以由多个组构成，不同的组之间不进行通信，一个组又相当于一个小的集群，组由多个 Storage Server 组成，组内的 Storage Server 会通过连接进行文件同步来保证高可用。 FastDFS上传文件 首先客户端请求Tracker服务获取到存储服务器的ip地址和端口，然后客户端根据返回的IP地址和端口号请求上传文件，存储服务器接收到请求后生产文件，并且将文件内容写入磁盘并返回给客户端file_id、路径信息、文件名等信息，客户端保存相关信息上传完毕。 FastDFS下载文件 客户端带上文件名信息请求Tracker服务获取到存储服务器的ip地址和端口，然后客户端根据返回的IP地址和端口号请求下载文件，存储服务器接收到请求后返回文件给客户端。 FastDFS 安装 条件有限，就将Tracker和Storage装在同一台服务器上了。 Tracker安装1.由于FastDFS是由C语言开发，所以需要安装gcc环境 1yum install gcc-c++ 2.安装依赖库 12345678910#安装libevent库yum -y install libevent cd /usr/local #安装libfastcommon库wget https://github.com/happyfish100/libfastcommon/archive/V1.0.43.tar.gz tar -zxvf V1.0.43.tar.gzcd libfastcommon-1.0.43/./make.sh./make.sh install 3.下载安装包 12345wget https://github.com/happyfish100/fastdfs/archive/V6.06.tar.gztar -zxvf V6.06.tar.gzcd fastdfs-6.06/./make.sh./make.sh install 安装成功后，执行如下命令，将安装目录内 conf 目录下的配置文件拷贝到 /etc/fdfs 目录下： 12cd conf&#x2F;cp .&#x2F;* &#x2F;etc&#x2F;fdfs&#x2F; 4.配置Tracker 接下来进入 /etc/fdfs/ 目录下进行配置： 打开 tracker.conf 文件： 1vi tracker.conf 修改如下配置： 123base_path&#x3D;&#x2F;home&#x2F;zmq&#x2F;fastdfs #tracker存储data和log的根路径，必须提前创建好port&#x3D;22122 #tracker默认22122,云服务器需要添加22122端口的实例组http.server_port&#x3D;80 #http端口，需要和nginx相同 5.启动 接下来执行如下命令启动 Tracker： 1&#x2F;usr&#x2F;bin&#x2F;fdfs_trackerd &#x2F;etc&#x2F;fdfs&#x2F;tracker.conf start 重启命令： 1&#x2F;usr&#x2F;bin&#x2F;fdfs_trackerd &#x2F;etc&#x2F;fdfs&#x2F;tracker.conf restart 如此之后，我们的 Tracker 就算安装成功了。 Storage安装 由于我将Storage和Tracker安装在同一台服务器上，相当于安装 Tracker 时已经安装了 Storage 了，所以这里只需要进行配置即可 1.配置Storage 进入/etc/fdfs目录，编辑storage.conf：vi storage.conf，修改相关参数： 12345base_path&#x3D;&#x2F;home&#x2F;zmq&#x2F;fastdfs #storage存储data和log的跟路径，必须提前创建好port&#x3D;23000 #storge默认23000，同一个组的storage端口号必须一致；云服务器需要添加23000端口的安全组store_path0&#x3D;&#x2F;home&#x2F;zmq&#x2F;fastdfs #如果为空，则使用base_pathtracker_server&#x3D;139.224.52.194:22122 #配置该storage监听的tracker的ip和porthttp.server_port&#x3D;80 #http端口，需要和nginx相同 这里有个坑，如果是云服务器，在服务器上自测时，要把tracker_server地址改为服务器内网地址，如果使用java客户端上传时，需要将tracker_server改为服务器外网地址。 配置完成后，执行如下命令启动 Storage： 1&#x2F;usr&#x2F;bin&#x2F;fdfs_storaged &#x2F;etc&#x2F;fdfs&#x2F;storage.conf start 重启命令： 1&#x2F;usr&#x2F;bin&#x2F;fdfs_storaged &#x2F;etc&#x2F;fdfs&#x2F;storage.conf restart 查看端口情况： 1netstat -apn|grep fdfs 通过monitor来查看storage是否成功绑定： 1/usr/bin/fdfs_monitor /etc/fdfs/storage.conf 至此，storage服务器配置完成。 配置客户端编辑/etc/fdfs/client.conf文件,修改如下配置： 123base_path&#x3D;&#x2F;home&#x2F;zmq&#x2F;fastdfs tracker_server&#x3D;139.224.52.194:22122http.tracker_server_port&#x3D;80 配置完成后，进行初步测试：编写一个文件 #里面随便写点数据vim t.txt执行客户端上传命令进行测试： 1&#x2F;usr&#x2F;bin&#x2F;fdfs_upload_file &#x2F;etc&#x2F;fdfs&#x2F;client.conf t.txt 返回的路径就是文件的存储目录，在/home/zmq/fastdfs/data目录下 至此，测试成功想在web段查看上次存储的文件，需要安装nginx和fastdfs-nginx-module Nginx安装安装Nginx 首先下载 Nginx 1wget http:&#x2F;&#x2F;nginx.org&#x2F;download&#x2F;nginx-1.17.0.tar.gz 然后解压下载的目录，进入解压目录中，在编译安装之前，需要安装两个依赖： 12yum -y install pcre-develyum -y install openssl openssl-devel 然后开始编译安装： 123.&#x2F;configuremakemake install 装好之后，默认安装位置在 ： 1&#x2F;usr&#x2F;local&#x2F;nginx&#x2F;sbin&#x2F;nginx 进入该目录的conf目录中，修改nginx.conf中的server_name和port。 进入到该目录的 sbin 目录下，执行 如下命令nginx ： 1.&#x2F;nginx 如果修改了 Nginx 配置，则可以通过如下命令重新加载 Nginx 配置文件： 1.&#x2F;nginx -s reload Nginx 启动成功之后，在浏览器中直接访问 Nginx 地址： 看到如上页面，表示 Nginx 已经安装成功了。如果访问不到，检查一下安全组和防火墙有没有添加80端口。 安装fastdfs-nginx-module12345cd &#x2F;usr&#x2F;local#下载fastdfs-nginx-modulewget https:&#x2F;&#x2F;github.com&#x2F;happyfish100&#x2F;fastdfs-nginx-module&#x2F;archive&#x2F;V1.22.tar.gz#解压tar -zxvf V1.22.tar.gz 然后将 /usr/local/fastdfs-nginx-module-1.22/src/mod_fastdfs.conf 文件拷贝到 /etc/fdfs/ 目录下，并修改该文件的内容,将tracker_server改为对应的ip，将url_hava_group_name改为true，store_path0保持和storage.conf一样 12cp &#x2F;usr&#x2F;local&#x2F;fastdfs-nginx-module-1.22&#x2F;src&#x2F;mod_fastdfs.conf &#x2F;etc&#x2F;fdfs&#x2F;vi &#x2F;etc&#x2F;fdfs&#x2F;mod_fastdfs.conf 接下来，回到下载的 nginx 安装文件的解压目录中，执行如下命令，重新配置编译安装： 123.&#x2F;configure --add-module&#x3D;&#x2F;usr&#x2F;local&#x2F;fastdfs-nginx-module-1.22&#x2F;srcmakemake install 安装完成后，修改 nginx 的配置文件，如下： 1vi &#x2F;usr&#x2F;local&#x2F;nginx&#x2F;conf&#x2F;nginx.conf 这里还有一点需要注意，需要将用户注释放开并改为root，否则访问不到你的资源。 再进入sbin启动Nginx 12cd /usr/local/nginx/sbin./nginx -s reload 通过浏览器去访问我上传的文件发现是404，查看nginx日志发现nginx 的错误日志中一直报： unknown directive &quot;ngx_fastdfs_module&quot; in /usr/local/nginx/conf/nginx.conf:70，这里可折腾了我好久，上网查了下发现原来需要用： systemctl restart nginx 命令重启nginx，如果只是重新加载 nginx.conf 文件（nginx -s reload）是不管用的。。。 访问结果： 终于大功告成！","categories":[{"name":"FastDFS","slug":"FastDFS","permalink":"https://zmq-git.github.io/categories/FastDFS/"}],"tags":[{"name":"教程","slug":"教程","permalink":"https://zmq-git.github.io/tags/%E6%95%99%E7%A8%8B/"}]},{"title":"MyBatis之大批量数据处理","slug":"Mybatis之大批量数据处理","date":"2021-11-22T09:29:37.000Z","updated":"2021-12-02T01:48:50.637Z","comments":true,"path":"2021/11/22/Mybatis之大批量数据处理/","link":"","permalink":"https://zmq-git.github.io/2021/11/22/Mybatis%E4%B9%8B%E5%A4%A7%E6%89%B9%E9%87%8F%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86/","excerpt":"","text":"在遇到批量插入数据的场景时，有几种方法可以使用，第一种方法是直接使用for循环遍历插入，第二种方法是使用MyBatis自带的foreach标签拼接sql，第三种是使用MyBatis中的批处理功能，那么在需要处理大量数据时，到底选择哪种方法更好呢？第一种方法显然不行，反复获取连接，释放连接，网络IO等等都需要消耗大量时间，下面我来验证下第二种和第三种处理十万条数据时哪个效率更快。 前期准备创建一个测试表 12345CREATE TABLE `user` ( `id` int(11) unsigned NOT NULL AUTO_INCREMENT, `pattern` varchar(255) DEFAULT NULL, PRIMARY KEY (`id`)) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4; 然后新建一个SpringBoot工程，数据库配置信息如下： 1234spring.datasource.url&#x3D;jdbc:mysql:&#x2F;&#x2F;localhost:3306&#x2F;test?useUnicode&#x3D;true&amp;characterEncoding&#x3D;utf8&amp;useSSL&#x3D;false&amp;serverTimezone&#x3D;UTC&amp;rewriteBatchedStatements&#x3D;truespring.datasource.username&#x3D;rootspring.datasource.password&#x3D;zhangmqspring.datasource.driverClassName&#x3D;com.mysql.cj.jdbc.Driver 这个数据库连接 URL 地址中多了一个参数 rewriteBatchedStatements，这是核心。 MySQL JDBC 驱动在默认情况下会无视 executeBatch() 语句，把我们期望批量执行的一组 sql 语句拆散，一条一条地发给 MySQL 数据库，批量插入实际上是单条插入，直接造成较低的性能。将 rewriteBatchedStatements 参数置为 true, 数据库驱动才会帮我们批量执行 SQL。 创建mapper,这里因为配置了@MapperScan所以不需要再单独添加@Mapper注解了 1234567public interface MenuMapper &#123; //批处理 void batchInsert(Menu menu); //foreach void foreachInsert(@Param(&quot;menus&quot;) List&lt;Menu&gt; menus);&#125; 创建对应xml： 123456789101112131415&lt;?xml version&#x3D;&quot;1.0&quot; encoding&#x3D;&quot;UTF-8&quot; ?&gt;&lt;!DOCTYPE mapper PUBLIC &quot;-&#x2F;&#x2F;mybatis.org&#x2F;&#x2F;DTD Mapper 3.0&#x2F;&#x2F;EN&quot; &quot;http:&#x2F;&#x2F;mybatis.org&#x2F;dtd&#x2F;mybatis-3-mapper.dtd&quot; &gt;&lt;mapper namespace&#x3D;&quot;com.example.mapper.MenuMapper&quot;&gt; &lt;insert id&#x3D;&quot;batchInsert&quot; parameterType&#x3D;&quot;com.example.entity.Menu&quot;&gt; insert into menu(pattern) values(#&#123;pattern&#125;) &lt;&#x2F;insert&gt; &lt;insert id&#x3D;&quot;foreachInsert&quot;&gt; insert into menu(pattern) values &lt;foreach collection&#x3D;&quot;menus&quot; item&#x3D;&quot;item&quot; separator&#x3D;&quot;,&quot;&gt; (#&#123;item.pattern&#125;) &lt;&#x2F;foreach&gt; &lt;&#x2F;insert&gt;&lt;&#x2F;mapper&gt; service如下： 12345678910111213141516171819202122232425262728293031323334353637383940@Servicepublic class MenuService &#123; @Autowired MenuMapper menuMapper; @Autowired SqlSessionFactory sqlSessionFactory; &#x2F;&#x2F;foreach插入 public void insert()&#123; List&lt;Menu&gt; list &#x3D; initData(100000); long start &#x3D; System.currentTimeMillis(); menuMapper.foreachInsert(list); long end &#x3D; System.currentTimeMillis(); System.out.println(&quot;合并成一条 SQL 插入耗费时间&quot;+(end-start)); &#125; &#x2F;&#x2F;批量插入 public void batchInsert()&#123; List&lt;Menu&gt; list &#x3D; initData(100000); SqlSession session &#x3D; sqlSessionFactory.openSession(ExecutorType.BATCH); MenuMapper mapper &#x3D; session.getMapper(MenuMapper.class); long start &#x3D; System.currentTimeMillis(); list.forEach(item -&gt;&#123; mapper.batchInsert(item); &#125;); session.commit(); long end &#x3D; System.currentTimeMillis(); System.out.println(&quot;一条条插入 SQL 耗费时间&quot;+(end-start)); &#125; &#x2F;&#x2F;提供数据 public List initData(int num)&#123; List&lt;Menu&gt; list &#x3D; new ArrayList(); for(int i &#x3D; 0; i &lt; num; i++)&#123; Menu menu &#x3D; new Menu(); menu.setPattern(&quot;test&quot;); list.add(menu); &#125; return list; &#125;&#125; 测试使用foreach123456789101112@SpringBootTestclass BatchInsertApplicationTests &#123; @Autowired MenuService menuService; @Test void contextLoads() &#123; menuService.insert(); &#125;&#125; 第一次： 第二次： 第三次： 使用foreach插入十万条数据，执行三次的平均耗时为6293ms。 使用batch批处理123456789101112@SpringBootTestclass BatchInsertApplicationTests &#123; @Autowired MenuService menuService; @Test void contextLoads() &#123; menuService.batchInsert(); &#125;&#125; 第一次： 第二次： 第三次： 使用批处理插入十万条数据，执行三次的平均耗时为5135ms 总结 插入方式 数据量 执行三次的平均耗时 foreach拼接sql 10w 6293ms 批处理 10w 5135ms 循环插入单条数据虽然效率极低，但是代码量极少，数据量较小时可以使用，但是数据量较大禁止使用，效率太低了； foreach 拼接sql的方式，使用时有大段的xml和sql语句要写，很容易出错，虽然效率尚可，但是真正应对大量数据的时候，拼接后的sql有可能会触发数据库最大执行sql的限制，所以不推荐使用； 批处理执行是有大数据量插入时推荐的做法，使用起来也比较方便。","categories":[{"name":"mybatis","slug":"mybatis","permalink":"https://zmq-git.github.io/categories/mybatis/"}],"tags":[{"name":"学习笔记","slug":"学习笔记","permalink":"https://zmq-git.github.io/tags/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"}]},{"title":"Java线程池学习","slug":"Java线程池学习","date":"2021-09-03T06:23:10.000Z","updated":"2021-09-07T09:07:35.855Z","comments":true,"path":"2021/09/03/Java线程池学习/","link":"","permalink":"https://zmq-git.github.io/2021/09/03/Java%E7%BA%BF%E7%A8%8B%E6%B1%A0%E5%AD%A6%E4%B9%A0/","excerpt":"","text":"使用线程池的好处 降低资源消耗。通过重复利用已创建的线程降低线程创建和销毁造成的消耗。 提高响应速度。当任务到达时，任务可以不需要的等到线程创建就能立即执行。 提高线程的可管理性。线程是稀缺资源，如果无限制的创建，不仅会消耗系统资源，还会降低系统的稳定性，使用线程池可以进行统一的分配，调优和监控。 ThreadPoolExecutor 类线程池实现类 ThreadPoolExecutor 是 Executor 框架最核心的类。查看它的构造方法： 12345678910111213141516171819202122232425 /** * 用给定的初始参数创建一个新的ThreadPoolExecutor。 */ public ThreadPoolExecutor(int corePoolSize,//线程池的核心线程数量 int maximumPoolSize,//线程池的最大线程数 long keepAliveTime,//当线程数大于核心线程数时，多余的空闲线程存活的最长时间 TimeUnit unit,//时间单位 BlockingQueue&lt;Runnable&gt; workQueue,//任务队列，用来储存等待执行任务的队列 ThreadFactory threadFactory,//线程工厂，用来创建线程，一般默认即可 RejectedExecutionHandler handler//拒绝策略，当提交的任务过多而不能及时处理时，我们可以定制策略来处理任务) &#123; if (corePoolSize &lt; 0 || maximumPoolSize &lt;= 0 || maximumPoolSize &lt; corePoolSize || keepAliveTime &lt; 0) throw new IllegalArgumentException(); if (workQueue == null || threadFactory == null || handler == null) throw new NullPointerException(); this.corePoolSize = corePoolSize; this.maximumPoolSize = maximumPoolSize; this.workQueue = workQueue; this.keepAliveTime = unit.toNanos(keepAliveTime); this.threadFactory = threadFactory; this.handler = handler; &#125; ThreadPoolExecutor 3 个最重要的参数： corePoolSize : 核心线程数线程数定义了最小可以同时运行的线程数量。 maximumPoolSize : 当队列中存放的任务达到队列容量的时候，当前可以同时运行的线程数量变为最大线程数。 workQueue: 当新任务来的时候会先判断当前运行的线程数量是否达到核心线程数，如果达到的话，新任务就会被存放在队列中。 ThreadPoolExecutor其他常见参数: keepAliveTime:当线程池中的线程数量大于 corePoolSize 的时候，如果这时没有新的任务提交，核心线程外的线程不会立即销毁，而是会等待，直到等待的时间超过了 keepAliveTime才会被回收销毁； unit : keepAliveTime 参数的时间单位。 threadFactory :executor 创建新线程的时候会用到。 handler :饱和策略； ThreadPoolExecutor 饱和策略定义: 如果当前同时运行的线程数量达到最大线程数量并且队列也已经被放满了任务时，ThreadPoolTaskExecutor 定义一些策略: ThreadPoolExecutor.AbortPolicy ：抛出 RejectedExecutionException来拒绝新任务的处理。 ThreadPoolExecutor.CallerRunsPolicy ：调用执行自己的线程运行任务，也就是直接在调用execute方法的线程中运行(run)被拒绝的任务，如果执行程序已关闭，则会丢弃该任务。因此这种策略会降低对于新任务提交速度，影响程序的整体性能。如果您的应用程序可以承受此延迟并且你要求任何一个任务请求都要被执行的话，你可以选择这个策略。 ThreadPoolExecutor.DiscardPolicy ：不处理新任务，直接丢弃掉。 ThreadPoolExecutor.DiscardOldestPolicy ： 此策略将丢弃最早的未处理的任务请求。 线程池的处理流程 几个常见的对比Runnable vs CallableRunnable自 Java 1.0 以来一直存在，但Callable仅在 Java 1.5 中引入,目的就是为了来处理Runnable不支持的用例。**Runnable 接口不会返回结果或抛出检查异常，但是 Callable 接口可以。所以，如果任务不需要返回结果或抛出异常推荐使用 **Runnable 接口，这样代码看起来会更加简洁。 execute() vs submit() execute()方法用于提交不需要返回值的任务，所以无法判断任务是否被线程池执行成功与否； submit()方法用于提交需要返回值的任务。线程池会返回一个 Future 类型的对象，通过这个 Future 对象可以判断任务是否执行成功 ，并且可以通过 Future 的 get()方法来获取返回值，get()方法会阻塞当前线程直到任务完成，而使用 get（long timeout，TimeUnit unit）方法则会阻塞当前线程一段时间后立即返回，这时候有可能任务没有执行完。 shutdown()VSshutdownNow() shutdown（） :关闭线程池，线程池的状态变为 SHUTDOWN。线程池不再接受新任务了，但是队列里的任务得执行完毕。 shutdownNow（） :关闭线程池，线程的状态变为 STOP。线程池会终止当前正在运行的任务，并停止处理排队的任务并返回正在等待执行的 List。 isTerminated() VS isShutdown() isShutDown 当调用 shutdown() 方法后返回为 true。 isTerminated 当调用 shutdown() 方法后，并且所有提交的任务完成后返回为 true","categories":[{"name":"Java","slug":"Java","permalink":"https://zmq-git.github.io/categories/Java/"}],"tags":[{"name":"学习笔记","slug":"学习笔记","permalink":"https://zmq-git.github.io/tags/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"}]},{"title":"Autowired和Resouce的区别","slug":"Autowired和Resource的区别","date":"2021-08-05T02:22:41.000Z","updated":"2021-08-05T03:08:40.832Z","comments":true,"path":"2021/08/05/Autowired和Resource的区别/","link":"","permalink":"https://zmq-git.github.io/2021/08/05/Autowired%E5%92%8CResource%E7%9A%84%E5%8C%BA%E5%88%AB/","excerpt":"","text":"@Autowired和@Resouce的区别​ @Autowired功能虽说非常强大，但是也有些不足之处。比如：比如它跟spring强耦合了，如果换成了JFinal等其他框架，功能就会失效。而@Resource是JSR-250提供的，它是Java标准，绝大部分框架都支持。 除此之外，有些场景使用@Autowired无法满足的要求，改成@Resource却能解决问题。接下来，我们重点看看@Autowired和@Resource的区别。 @Autowired默认按byType自动装配，而@Resource默认byName自动装配。 @Autowired只包含一个参数：required，表示是否开启自动准入，默认是true。而@Resource包含七个参数，其中最重要的两个参数是：name 和 type。 @Autowired如果要使用byName，需要使用@Qualifier一起配合。而@Resource如果指定了name，则用byName自动装配，如果指定了type，则用byType自动装配。 @Autowired能够用在：构造器、方法、参数、成员变量和注解上，而@Resource能用在：类、成员变量和方法上。 @Autowired是spring定义的注解，而@Resource是JSR-250定义的注解。 此外，它们的装配顺序不同。 @Autowired的装配顺序如下：​ @Resource的装配顺序如下： 如果同时指定了name和type： 2.如果指定了name： ​ 3.如果指定了type ​ 4.如果既没指定name也没指定type ​","categories":[{"name":"SpringBoot","slug":"SpringBoot","permalink":"https://zmq-git.github.io/categories/SpringBoot/"}],"tags":[{"name":"学习笔记","slug":"学习笔记","permalink":"https://zmq-git.github.io/tags/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"}]},{"title":"说说springboot中的@SpringBootApplication注解","slug":"关于springboot中的-SpringBootApplicationn注解","date":"2021-07-19T08:54:08.000Z","updated":"2021-07-20T01:31:39.397Z","comments":true,"path":"2021/07/19/关于springboot中的-SpringBootApplicationn注解/","link":"","permalink":"https://zmq-git.github.io/2021/07/19/%E5%85%B3%E4%BA%8Espringboot%E4%B8%AD%E7%9A%84-SpringBootApplicationn%E6%B3%A8%E8%A7%A3/","excerpt":"","text":"spring boot 它的设计目的就是为例简化开发，开启了各种自动装配，你不想写各种配置文件，引入相关的依赖就能迅速搭建起一个web工程。它采用的是建立生产就绪spring应用程序观点，约定优先于配置的惯例。 我们会发现所有的SpringBoot项目的启动类上都会有一个@SpringBootApplication注解，在这个注解中，有三个最重要的注解： 1. @SpringBootConfiguration​ @SpringBootConfiguration继承自@Configuration，二者功能也一致，标注当前类是配置类，并会将当前类内声明的一个或多个以@Bean注解标记的方法的实例纳入到spring容器中，并且实例名就是方法名。 2. @EnableAutoConfiguration​ 从字面上看是自动配置的意思，为什么SpringBoot不需要像SSM,SSH那样书写繁多的配置文件就能达到目的呢？答案就在这里。 ​ 通过查看源码我们可以发现，该注解上主要有两个注解在起作用： @AutoConfigurationPackage注解的作用是将 添加该注解的类所在的package 作为 自动配置package 进行管理，也就是说当SpringBoot应用启动时默认会将启动类所在的package作为自动配置的package。 @Import注解的作用是注册bean对象，起着和@Component和@Configuration+@Bean一样的作用，AutoConfigurationImportSelector的作用是扫描META-INF目录下面的spring.factories文件，返回里面配置类的权限类名数组，配合@Import注解将这些类注册到spring容器里面。 3. @ComponentScan​ 这个注解起到了包扫描的作用，默认扫描当前类所在的包以及所在的子包。 ​ 我们通过查看源码可以发现，@ComponentScan里面有个@excludeFilters注解，里面去除了两个过滤器：AutoConfigurationExcludeFilter的作用是声明不用去扫描自动配置的类，因为在@EnableAutoConfiguration注解中已经说到，这个注解就是去扫描所有的配置类，所有这里不用进行二次扫描。 利用TypeExcludeFilter我们可以自定义不扫描哪些类，继承TypeExcludeFilter重写match方法，在该方法里我们可以自定义规则。","categories":[{"name":"SpringBoot","slug":"SpringBoot","permalink":"https://zmq-git.github.io/categories/SpringBoot/"}],"tags":[{"name":"学习笔记","slug":"学习笔记","permalink":"https://zmq-git.github.io/tags/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"}]},{"title":"SpringBoot中的静态变量赋值","slug":"SpringBoot中的静态变量赋值","date":"2021-04-16T13:26:26.000Z","updated":"2021-12-07T02:06:41.666Z","comments":true,"path":"2021/04/16/SpringBoot中的静态变量赋值/","link":"","permalink":"https://zmq-git.github.io/2021/04/16/SpringBoot%E4%B8%AD%E7%9A%84%E9%9D%99%E6%80%81%E5%8F%98%E9%87%8F%E8%B5%8B%E5%80%BC/","excerpt":"","text":"最近在SpringBoot项目中需要利用到FTP实现上传下载，于是我统一封装了一个FTP工具类，既然是工具类，那肯定是写成静态方法，而不是通过实例调用，但是通过@Value注解获取配置文件属性（问题一）以及在静态方法里使用@Resource注入的bean对象（问题二）时，遇到了问题。 问题一描述我将FTP配置放在了配置文件application.yml中，在FtpUtils工具类中，使用@Value获取配置 1234567891011@Componentpublic class FtpUtil &#123; @Value(&quot;$&#123;ftp.username&#125;&quot;) private static String username; &#x2F;&#x2F; FTP 登录用户名 @Value(&quot;$&#123;ftp.username&#125;&quot;) private static String password; &#x2F;&#x2F; FTP 登录密码 @Value(&quot;$&#123;ftp.username&#125;&quot;) private static String ip; &#x2F;&#x2F; FTP 服务器地址IP地址 @Value(&quot;$&#123;ftp.username&#125;&quot;) private static int port; &#x2F;&#x2F; FTP 端口 ... 当运行时，发现拿不到FTP连接，经过debug调试发现，FTP配置属性全为空，查阅资料说，静态变量是类属性，而spring的注解都是针对对象的。所以不能赋值成功。 解决方案可以这样写： 123456789101112131415161718192021222324252627@Componentpublic class FtpUtil &#123; private static String username; &#x2F;&#x2F; FTP 登录用户名 private static String password; &#x2F;&#x2F; FTP 登录密码 private static String ip; &#x2F;&#x2F; FTP 服务器地址IP地址 private static int port; &#x2F;&#x2F; FTP 端口 @Value(&quot;$&#123;ftp.username&#125;&quot;) public void setUserName(String ftpUserName) &#123; username &#x3D; ftpUserName; &#125; @Value(&quot;$&#123;ftp.password&#125;&quot;) public void setPassword(String ftpPassword) &#123; password &#x3D; ftpPassword; &#125; @Value(&quot;$&#123;ftp.ip&#125;&quot;) public void setIp(String ftpIp) &#123; ip &#x3D; ftpIp; &#125; @Value(&quot;$&#123;ftp.port&#125;&quot;) public void setPort(int ftpPort) &#123; port &#x3D; ftpPort; &#125; ... ps: 一定要用@Component注册为bean对象，并且set方法不需要为静态方法。 问题二描述我想在静态方法中使用@Resource注入的bean对象，于是将其用static修饰: 12345@Componentpublic class FtpUtil &#123; @Resource private static IBsStaticDataSV iBsStaticDataSV; ... 运行时发现iBsStaticDataSV变量还是为空，为什么会出现这种情况？原因是Spring容器的依赖注入是依赖set方法，而set方法是实例对象的方法，注入依赖时是无法注入静态成员变量的，在调用的时候依赖的Bean才会为null； 解决方案使用@PostConstruct注解 123456789101112131415161718@Componentpublic class FtpUtil &#123; @Resource private IBsStaticDataSV iBsStaticDataSV; &#x2F;&#x2F;由于静态方法无法使用注入的Bean 定义静态变量 private static IBsStaticDataSV bsStaticDataSV; &#x2F;&#x2F;当容器实例化当前受管Bean时@PostConstruct注解的方法会被自动触发，借此来实现静态变量初始化 @PostConstruct public void init()&#123; bsStaticDataSV &#x3D; iBsStaticDataSV; &#125; public static String getFtpConnection()&#123; bsStaticDataSV.get(&quot;xxx&quot;) &#x2F;&#x2F;这里可以正常使用 &#125;","categories":[{"name":"SpringBoot","slug":"SpringBoot","permalink":"https://zmq-git.github.io/categories/SpringBoot/"}],"tags":[{"name":"Code","slug":"Code","permalink":"https://zmq-git.github.io/tags/Code/"}]},{"title":"mybatis的缓存机制","slug":"mybatis的缓存机制","date":"2021-04-04T13:48:50.000Z","updated":"2021-04-05T06:28:18.353Z","comments":true,"path":"2021/04/04/mybatis的缓存机制/","link":"","permalink":"https://zmq-git.github.io/2021/04/04/mybatis%E7%9A%84%E7%BC%93%E5%AD%98%E6%9C%BA%E5%88%B6/","excerpt":"","text":"mybatis分一级缓存和二级缓存，如下图，是mybatis一级缓存和二级缓存的区别图解： 一级缓存Mybatis的一级缓存是默认开启的，作用域是同一个SqlSession，当参数和SQL完全相同的情况下，使用同一个SqlSession对象调用同一个Mapper方法，当第1次执行SQL语句后，MyBatis会自动将其放在缓存中，后续再次查询时，如果没有声明需要刷新，且缓存没有超时，会直接取出此前缓存的数据，而不会再次发送SQL到数据库。从而提高查询效率。当 Session flush 或 close 之后，该Session中的所有 Cache 就将清空。 这里使用同一个sqlSession对象去查询两次，并且比较查询出来的对象是否是同一个，即比较内存地址。 12345678910111213141516171819202122232425public static void main(String[] args) throws Exception&#123; InputStream inputStream &#x3D; null; SqlSession sqlSession &#x3D; null; try &#123; inputStream &#x3D; Resources.getResourceAsStream(&quot;mybatis-config.xml&quot;); SqlSessionFactory sqlSessionFactory &#x3D; new SqlSessionFactoryBuilder().build(inputStream); sqlSession &#x3D; sqlSessionFactory.openSession(); UserDao userDao &#x3D; sqlSession.getMapper(UserDao.class); System.out.println(&quot;第一次查询&quot;); List&lt;User&gt; user1 &#x3D; userDao.getAllUser(); for (User user : user1) &#123; System.out.println(user); &#125; System.out.println(&quot;第二次查询&quot;); List&lt;User&gt; user2 &#x3D; userDao.getAllUser(); for (User user : user2) &#123; System.out.println(user); &#125; System.out.println(&quot;是同一个对象:&quot;+(user1&#x3D;&#x3D;user2)); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; finally &#123; closeResource(sqlSession, inputStream); &#125; &#125; 从下图可以看出，第一次查询打印了sql,说明第一次查询去数据库中查出来了数据，第二次查询没有去数据库中查询数据，而是直接从缓存中获取。并且两次查询的结果是同一个对象。 二级缓存Mybatis二级缓存的作用域是同一个Mapper，二级缓存开启后，同一个namespace下的所有操作语句，都影响着同一个Cache，即二级缓存被多个SqlSession共享，是一个全局的变量。所以二级存储也称之为namespace缓存！二级缓存需要进行配置开启。 ps：MyBatis默认实现的一二级缓存是使用HashMap存储的。只有当sqlSession关闭之后，该sqlSession的查询结果才会存入二级缓存中。 二级缓存的使用1.修改配置文件mybatis-config.xml加入&lt;setting name=”cacheEnabled”value=”true”/&gt;，由于默认是true，所以不添加也可以。 2.在mapper.xml中开启二缓存，mapper.xml下的sql执行完成会存储到它的缓存区,如： 3.对应的pojo实现序列化(implements Serializable)。 这里使用两个不同的sqlSession去获取mapper对象，调用同一个查询方法： 1234567891011121314151617181920212223242526272829public static void main(String[] args) &#123; InputStream inputStream &#x3D; null; SqlSession sqlSession1 &#x3D; null; SqlSession sqlSession2 &#x3D; null; try &#123; inputStream &#x3D; Resources.getResourceAsStream(&quot;mybatis-config.xml&quot;); SqlSessionFactory sqlSessionFactory &#x3D; new SqlSessionFactoryBuilder().build(inputStream); sqlSession1 &#x3D; sqlSessionFactory.openSession(); sqlSession2 &#x3D; sqlSessionFactory.openSession(); UserDao userDao1 &#x3D; sqlSession1.getMapper(UserDao.class); UserDao userDao2 &#x3D; sqlSession2.getMapper(UserDao.class); System.out.println(&quot;第一次查询&quot;); List&lt;User&gt; user1 &#x3D; userDao1.getAllUser(); for (User user : user1) &#123; System.out.println(user); &#125; System.out.println(&quot;第二次查询&quot;); List&lt;User&gt; user2 &#x3D; userDao2.getAllUser(); for (User user : user2) &#123; System.out.println(user); &#125; System.out.println(&quot;是同一个对象:&quot;+(user1&#x3D;&#x3D;user2)); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; finally &#123; closeResource(inputStream,sqlSession1,sqlSession2); &#125; &#125; 但是发现第二次查询还是去查询的数据库，并没有走缓存，因为第一个sqlSession1并没有关闭，查询结果并不会被序列化并保存到二级缓存，这里其实先去查询了二级缓存 Cache Hit Ratio [com.test.dao.UserDao]: 0.0），表示命中次数与查询次数的比值。 关闭sqlSession1，sqlSession1.close()，继续查看日志，可以看到第二次查询命中了二级缓存，命中一次/查询两次=0.5。 但是我们可以发现两次查询出来的对象不是同一个，难道不是查的缓存吗，其实是cache中的readOnly属性在启作用： readOnly 为只读属性， 默认为 false false: 可读写， 在创建对象时， 会通过反序列化得到缓存对象的拷贝。 因此在速度上会相对慢一点， 但重在安全。 true: 只读， 只读的缓存会给所有调用者返回缓存对象的相同实例。 因此性能很好， 但如果修改了对象， 有可能会导致程序出问题。 总结1.在开启二级缓存的情况下： 在同一个Mapper二级缓存里面，执行多次sql语句的情况是：首先查找二级缓存里面是否有数据，如果没有就找一级缓存，再没有，才会查找数据库。 2.在二级缓存没有开启的情况下： 查找数据的顺序是：查看一级缓存里面是否能够找到，找不到就在数据库里面找！！ 3.当为select语句时： flushCache默认为false，表示任何时候语句被调用，都不会去清空本地缓存和二级缓存。 useCache默认为true，表示会将本条语句的结果进行二级缓存。 4.当为insert、update、delete语句时： flushCache默认为true，表示任何时候语句被调用，都会导致本地缓存和二级缓存被清空。 注意事项由于在更新时会刷新缓存， 因此需要注意使用场合：查询频率很高， 更新频率很低时使用， 即经常使用 select, 相对较少使用delete, insert, update。 缓存是以 namespace 为单位的，不同 namespace 下的操作互不影响。但刷新缓存是刷新整个 namespace 的缓存， 也就是你 update 了一个， 则整个缓存都刷新了。 一二级缓存都可能会导致脏读（缓存不一样），二级缓存中，最好在 「只有单表操作」 的表的 namespace 使用缓存， 而且对该表的操作都在这个 namespace 中。 否则可能会出现数据不一致的情况（脏读）。","categories":[{"name":"mybatis","slug":"mybatis","permalink":"https://zmq-git.github.io/categories/mybatis/"}],"tags":[{"name":"学习笔记","slug":"学习笔记","permalink":"https://zmq-git.github.io/tags/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"}]},{"title":"Java中的String类","slug":"Java中的String类","date":"2021-03-31T06:11:30.000Z","updated":"2021-03-31T07:52:33.782Z","comments":true,"path":"2021/03/31/Java中的String类/","link":"","permalink":"https://zmq-git.github.io/2021/03/31/Java%E4%B8%AD%E7%9A%84String%E7%B1%BB/","excerpt":"","text":"String可变吗？1234567public class Test1 &#123; public static void main(String[] args) &#123; String str &#x3D; &quot;0000&quot;; str &#x3D; &quot;1111&quot;; System.out.println(str); &#125;&#125; ​ 我们都知道Java中的String是不可变的, 那上面段代码应该输出：0000, 这样才和不变性吻合。可实际上是这样吗, 并不是, 事实上输出的是：1111。那这是不是和不变性的说法冲突了呢？我们先来看下面一段话： 其实在JVM的运行中，会单独给一块地分给String。我们知道字符串的分配和其他对象分配一样，是需要消耗高昂的时间和空间的，而且字符串我们使用的非常多。JVM为了提高性能和减少内存的开销，在实例化字符串的时候进行了一些优化： 使用字符串常量池。每当我们创建字符串常量时，JVM会首先检查字符串常量池，如果该字符串已经存在常量池中，那么就直接返回常量池中的实例引用。如果字符串不存在常量池中，就会实例化该字符串并且将其放到常量池中。由于String字符串的不可变性我们可以十分肯定常量池中一定不存在两个相同的字符串。 ​ 在上面的代码中，首先会先去JVM的常量池中查找有没有”0000”这个对象，如果找到了，直接将该对象的引用地址赋值给str。找不到的话会创建一个对象，并且将其引用赋给str, str = “1111”也是同理。所以str实际上保存的是字符串在内存中的地址，str只是一个引用变量，真正不可变的是”0000”。 String为什么是不可变的?我们来看看他的源码 123456789public final class String implements java.io.Serializable, Comparable&lt;String&gt;, CharSequence &#123; &#x2F;** The value is used for character storage. *&#x2F; private final char value[]; public String(String original) &#123; this.value &#x3D; original.value; this.hash &#x3D; original.hash; &#125; 从上面的这段源码中可以看出三点： String 类是final修饰 String存储内容使用的是char数组 char数组是final修饰 所以，实际上我们在创建一个String对象的时候，内容被保存在了char数组中，而由于char数组被final修饰，所以不可变。 1234567891011public class Test1 &#123; public static void main(String[] args) &#123; &#x2F;&#x2F;String str &#x3D; &quot;0000&quot;;相当于 char[] value &#x3D; new char[]&#123;&#39;0&#39;,&#39;0&#39;,&#39;0&#39;,&#39;0&#39;&#125;; String str &#x3D; new String(value); &#x2F;&#x2F;str &#x3D; &quot;1111&quot;; char[] value1 &#x3D; new char[]&#123;&#39;1&#39;,&#39;1&#39;,&#39;1&#39;,&#39;1&#39;&#125;; str &#x3D; new String(value1); System.out.println(str); &#125;&#125; 这里复习一下，final有啥用 当用final修饰一个类时，表明这个类不能被继承。也就是说，如果一个类你永远不会让他被继承，就可以用final进行修饰。final类中的成员变量可以根据需要设为final，但是要注意final类中的所有成员方法都会被隐式地指定为final方法。 当final修饰的方法表示此方法已经是“最后的、最终的”含义，亦即此方法不能被重写（可以重载多个final修饰的方法）。此处需要注意的一点是：因为重写的前提是子类可以从父类中继承此方法，如果父类中final修饰的方法同时访问控制权限为private，将会导致子类中不能直接继承到此方法，因此，此时可以在子类中定义相同的方法名和参数，此时不再产生重写与final的矛盾，而是在子类中重新定义了新的方法。（注：类的private方法会隐式地被指定为final方法。） 当final修饰一个基本数据类型时，表示该基本数据类型的值一旦在初始化后便不能发生变化。如果final修饰一个引用类型时，则在对其初始化之后便不能再让其指向其他对象了，但该引用所指向的对象的内容是可以发生变化的。本质上是一回事，因为引用的值是一个地址，final要求值，即地址的值不发生变化。另外final修饰一个成员变量（属性），必须要显示初始化。这里有两种初始化方式，（在申明的时候给其赋值，否则必须在其类的所有构造方法中都要为其赋值） 再来看一个String的案例1234567891011public class Test1 &#123; public static void main(String[] args) &#123; String str &#x3D; &quot;Hello World&quot;; System.out.println(str); str.substring(0,str.indexOf(&quot;W&quot;)); System.out.println(str); System.out.println(str.substring(0,str.indexOf(&quot;W&quot;))); &#125;&#125; 输出： 无论是concat、replace、substring还是trim方法的操作都不是在原有的字符串上进行的，而是重新生成了一个新的字符串对象。也就是说进行这些操作后，最原始的字符串并没有被改变。这里踩过几个坑。。。 得出两个结论： String对象一旦被创建就是固定不变的了，对String对象的任何改变都不影响到原对象，相关的任何变化性的操作都会生成新的对象。 String对象每次有变化性操作的时候，都会重新new一个String对象（这里指的是有变化的情况）。 finally, 探究下String s = new String(“111”)会创建几个对象？接触Java后都知道可以new一个对象。所以 String s = new String(“111”);就是创建一个对象然后把对象引用地址赋给变量s。但是这里有个特殊点，那就是（“111”）,这里会先去JVM里的那块常量池里找找有没有这个对象，找到了直接将引用地址赋给String的构造方法。找不到就创建一个对象然后把引用地址给String的有参构造方法。 所以答案是: ​ 如果常量池中存在，则只需创建一个对象，否则需要创建两个对象。","categories":[{"name":"Java","slug":"Java","permalink":"https://zmq-git.github.io/categories/Java/"}],"tags":[{"name":"学习笔记","slug":"学习笔记","permalink":"https://zmq-git.github.io/tags/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"}]},{"title":"Springboot+Vue之DatePicker","slug":"Springboot+Vue之DatePicker","date":"2021-03-26T03:05:54.000Z","updated":"2021-03-26T09:03:07.261Z","comments":true,"path":"2021/03/26/Springboot+Vue之DatePicker/","link":"","permalink":"https://zmq-git.github.io/2021/03/26/Springboot+Vue%E4%B9%8BDatePicker/","excerpt":"","text":"最近在开发一个Springboot+Vue项目中，使用到Ant Design Vue中的日期选择框组件’DatePicker’，发现向后台提交数据时报错：Failed to convert property value of type ‘java.lang.String’ to required ‘Date’ 一开始想到的是在实体类属性上添加@DateTimeFormat注解，@DatetimeFormat是将String转换成Date，一般前台给后台传值时用 12@DateTimeFormat(pattern &#x3D; &quot;yyyy-MM-dd HH:mm:ss&quot;)private Date startDate; 发现还是报错： Failed to convert from type [java.lang.String] to type[@org.springframework.format.annotation.DateTimeFormat java.util.Date] for value ‘“2021-03-10T06:09:01.563Z” 表示无法将字符串转换为Date类型，查看日志发现前台传过来的格式并不是我在@DateTimeFormat 注解上pattern中的规定的格式 “2021-03-10T06:09:01.563Z”，而@DateTimeFormat 注解只能解析与它定义的格式相同的字符串，所以导致报错。在前台通过F12打印出日期，发现最终提交到后台的时间格式为Moment()，并不是String，解决办法： 1.引入moment 1import moment from &#39;moment&#39; 2.修改vue代码，更改传入后台的Date的时间格式 1moment(date).format(&#39;YYYY-MM-DD HH:mm:ss&#39;);","categories":[{"name":"Web","slug":"Web","permalink":"https://zmq-git.github.io/categories/Web/"}],"tags":[{"name":"Code","slug":"Code","permalink":"https://zmq-git.github.io/tags/Code/"}]},{"title":"浅谈hashCode()和equals()","slug":"hashCode-和equals","date":"2021-03-23T08:42:04.000Z","updated":"2021-03-31T07:44:56.106Z","comments":true,"path":"2021/03/23/hashCode-和equals/","link":"","permalink":"https://zmq-git.github.io/2021/03/23/hashCode-%E5%92%8Cequals/","excerpt":"","text":"equals()介绍：equals() 的作用是 用来判断两个对象是否相等。 equals() 定义在JDK的Object.java中。通过判断两个对象的地址是否相等(即，是否是同一个对象)来区分它们是否相等。源码如下： 123public boolean equals(Object obj) &#123; return (this &#x3D;&#x3D; obj);&#125; 既然Object.java中定义了equals()方法，这就意味着所有的Java类都实现了equals()方法，所有的类都可以通过equals()去比较两个对象是否相等。 但是，我们已经说过，使用默认的“**equals()”方法，等价于“==**”方法。因此，我们通常会重写equals()方法：若两个对象的内容相等，则equals()方法返回true；否则，返回fasle。 下面根据“类是否覆盖equals()方法”，将它分为2类。(01) 若某个类没有覆盖equals()方法，当它的通过equals()比较两个对象时，实际上是比较两个对象是不是同一个对象。这时，等价于通过“==”去比较这两个对象。(02) 我们可以覆盖类的equals()方法，来让equals()通过其它方式比较两个对象是否相等。通常的做法是：若两个对象的内容相等，则equals()方法返回true；否则，返回fasle。 java对equals()的要求，有以下几点： 对称性：如果x.equals(y)返回是”true”，那么y.equals(x)也应该返回是”true”。 反射性：x.equals(x)必须返回是”true”。 类推性：如果x.equals(y)返回是”true”，而且y.equals(z)返回是”true”，那么z.equals(x)也应该返回是”true”。 一致性：如果x.equals(y)返回是”true”，只要x和y内容一直不变，不管你重复x.equals(y)多少次，返回都是”true”。 非空性，x.equals(null)，永远返回是”false”；x.equals(和x不同类型的对象)永远返回是”false”。 hashCode()介绍:hashCode() 的作用是获取哈希码，也称为散列码；它实际上是返回一个 int 整数。这个哈希码的作用是确定该对象在哈希表中的索引位置。hashCode()定义在 JDK 的 Object 类中，这就意味着 Java 中的任何类都包含有 hashCode() 函数。另外需要注意的是： Object 的 hashcode 方法是本地方法，也就是用 c 语言或 c++ 实现的，该方法通常用来将对象的 内存地址 转换为整数之后返回。 1public native int hashCode();Copy to clipboardErrorCopied 散列表存储的是键值对(key-value)，它的特点是：能根据“键”快速的检索出对应的“值”。这其中就利用到了散列码！（可以快速找到所需要的对象） 为什么要有 hashCode？我们以“HashSet 如何检查重复”为例子来说明为什么要有 hashCode？ 当你把对象加入 HashSet 时，HashSet 会先计算对象的 hashcode 值来判断对象加入的位置，同时也会与其他已经加入的对象的 hashcode 值作比较，如果没有相符的 hashcode，HashSet 会假设对象没有重复出现。但是如果发现有相同 hashcode 值的对象，这时会调用 equals() 方法来检查 hashcode 相等的对象是否真的相同。如果两者相同，HashSet 就不会让其加入操作成功。如果不同的话，就会重新散列到其他位置。（摘自 Java 启蒙书《Head First Java》第二版）。这样我们就大大减少了 equals 的次数，相应就大大提高了执行速度。 为什么重写 equals 时必须重写 hashCode 方法？如果两个对象相等，则 hashcode 一定也是相同的。两个对象相等,对两个对象分别调用 equals 方法都返回 true。但是，两个对象有相同的 hashcode 值，它们也不一定是相等的 。因此，equals 方法被覆盖过，则 hashCode 方法也必须被覆盖。 hashCode()的默认行为是对堆上的对象产生独特值。如果没有重写 hashCode()，则该 class 的两个对象无论如何都不会相等（即使这两个对象指向相同的数据） 为什么两个对象有相同的 hashcode 值，它们也不一定是相等的？因为 hashCode() 所使用的杂凑算法也许刚好会让多个对象传回相同的杂凑值。越糟糕的杂凑算法越容易碰撞，但这也与数据值域分布的特性有关（所谓碰撞也就是指的是不同的对象得到相同的 hashCode。 我们刚刚也提到了 HashSet,如果 HashSet 在对比的时候，同样的 hashcode 有多个对象，它会使用 equals() 来判断是否真的相同。也就是说 hashcode 只是用来缩小查找成本。","categories":[{"name":"Java","slug":"Java","permalink":"https://zmq-git.github.io/categories/Java/"}],"tags":[{"name":"学习笔记","slug":"学习笔记","permalink":"https://zmq-git.github.io/tags/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"}]},{"title":"Mysql之事务","slug":"mysql之事务","date":"2021-03-21T11:54:23.994Z","updated":"2021-03-31T07:54:36.771Z","comments":true,"path":"2021/03/21/mysql之事务/","link":"","permalink":"https://zmq-git.github.io/2021/03/21/mysql%E4%B9%8B%E4%BA%8B%E5%8A%A1/","excerpt":"","text":"什么是事务？事务是逻辑上的一组操作，要么都执行，要么都不执行。 假如小明要给小红转账1000元，这个转账会涉及到两个关键操作就是：将小明的余额减少1000元，将小红的余额增加1000元。万一在这两个操作之间突然出现错误比如银行系统崩溃，导致小明余额减少而小红的余额没有增加，这样就不对了。事务就是保证这两个关键操作要么都成功，要么都要失败。 事务的四大特性(ACID) 原子性（Atomicity）： 事务是最小的执行单位，不允许分割。事务的原子性确保动作要么全部完成，要么完全不起作用； 一致性（Consistency）： 执行事务后，数据库从一个正确的状态变化到另一个正确的状态； 隔离性（Isolation）： 并发访问数据库时，一个用户的事务不被其他事务所干扰，各并发事务之间数据库是独立的； 持久性（Durability）： 一个事务被提交之后。它对数据库中数据的改变是持久的，即使数据库发生故障也不应该对其有任何影响。 并发事务带来哪些问题在典型的应用程序中，多个事务并发运行，经常会操作相同的数据来完成各自的任务（多个用户对同一数据进行操作）。并发虽然是必须的，但可能会导致以下的问题。 脏读（Dirty read）: 当一个事务正在访问数据并且对数据进行了修改，而这种修改还没有提交到数据库中，这时另外一个事务也访问了这个数据，然后使用了这个数据。因为这个数据是还没有提交的数据，那么另外一个事务读到的这个数据是“脏数据”，依据“脏数据”所做的操作可能是不正确的。 丢失修改（Lost to modify）: 指在一个事务读取一个数据时，另外一个事务也访问了该数据，那么在第一个事务中修改了这个数据后，第二个事务也修改了这个数据。这样第一个事务内的修改结果就被丢失，因此称为丢失修改。 例如：事务1读取某表中的数据A=20，事务2也读取A=20，事务1修改A=A-1，事务2也修改A=A-1，最终结果A=19，事务1的修改被丢失。 不可重复读（Unrepeatableread）: 指在一个事务内多次读同一数据。在这个事务还没有结束时，另一个事务也访问该数据。那么，在第一个事务中的两次读数据之间，由于第二个事务的修改导致第一个事务两次读取的数据可能不太一样。这就发生了在一个事务内两次读到的数据是不一样的情况，因此称为不可重复读。 幻读（Phantom read）: 幻读与不可重复读类似。它发生在一个事务（T1）读取了几行数据，接着另一个并发事务（T2）插入了一些数据时。在随后的查询中，第一个事务（T1）就会发现多了一些原本不存在的记录，就好像发生了幻觉一样，所以称为幻读。 不可重复读和幻读区别： 不可重复读的重点是修改比如多次读取一条记录发现其中某些列的值被修改，幻读的重点在于新增或者删除比如多次读取一条记录发现记录增多或减少了。 事务隔离级别有哪些?MySQL的默认隔离级别是?SQL 标准定义了四个隔离级别： READ-UNCOMMITTED(读取未提交)： 最低的隔离级别，允许读取尚未提交的数据变更，可能会导致脏读、幻读或不可重复读。 READ-COMMITTED(读取已提交)： 允许读取并发事务已经提交的数据，可以阻止脏读，但是幻读或不可重复读仍有可能发生。 REPEATABLE-READ(可重复读)： 对同一字段的多次读取结果都是一致的，除非数据是被本身事务自己所修改，可以阻止脏读和不可重复读，但幻读仍有可能发生。 SERIALIZABLE(可串行化)： 最高的隔离级别，完全服从ACID的隔离级别。所有的事务依次逐个执行，这样事务之间就完全不可能产生干扰，也就是说，该级别可以防止脏读、不可重复读以及幻读。 隔离级别 脏读 不可重复读 幻影读 READ-UNCOMMITTED √ √ √ READ-COMMITTED × √ √ REPEATABLE-READ × × √ SERIALIZABLE × × × 隔离级别越低，事务请求的锁越少；MySQL InnoDB 存储引擎的默认支持的隔离级别是 REPEATABLE-READ（可重读）。我们可以通过SELECT @@tx_isolation;命令来查看；大部分数据库的隔离级别都是READ-COMMITTED(读取已提交)，InnoDB 存储引擎默认使用 REPEAaTABLE-READ（可重读） 并不会有任何性能损失。","categories":[{"name":"Mysql","slug":"Mysql","permalink":"https://zmq-git.github.io/categories/Mysql/"}],"tags":[{"name":"学习笔记","slug":"学习笔记","permalink":"https://zmq-git.github.io/tags/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"}]},{"title":"Redis集群","slug":"搭建Redis集群","date":"2021-03-20T07:33:08.016Z","updated":"2021-04-03T13:19:12.078Z","comments":true,"path":"2021/03/20/搭建Redis集群/","link":"","permalink":"https://zmq-git.github.io/2021/03/20/%E6%90%AD%E5%BB%BARedis%E9%9B%86%E7%BE%A4/","excerpt":"","text":"redis集群简介 是一个开源的key value存储系统，受到了广大互联网公司的青睐。redis3.0版本之前只支持单例模式，在3.0版本及以后才支持集群； redis集群采用P2P模式，是完全去中心化的，不存在中心节点或者代理节点； redis集群是没有统一的入口的，客户端（client）连接集群的时候连接集群中的任意节点（node）即可，集群内部的节点是相互通信的（PING-PONG机制），每个节点都是一个redis实例； 为了实现集群的高可用，即判断节点是否健康（能否正常使用），redis-cluster有这么一个投票容错机制：如果集群中超过半数的节点投票认为某个节点挂了，那么这个节点就挂了（fail）。这是判断节点是否挂了的方法； 那么如何判断集群是否挂了呢? -&gt; 如果集群中任意一个节点挂了，而且该节点没有从节点（备份节点），那么这个集群就挂了。这是判断集群是否挂了的方法； 那么为什么任意一个节点挂了（没有从节点）这个集群就挂了呢？ -&gt; 因为集群内置了16384个slot（哈希槽），并且把所有的物理节点映射到了这16384[0-16383]个slot上，或者说把这些slot均等的分配给了各个节点。当需要在Redis集群存放一个数据（key-value）时，redis会先对这个key进行crc16算法，然后得到一个结果。再把这个结果对16384进行求余，这个余数会对应[0-16383]其中一个槽，进而决定key-value存储到哪个节点中。所以一旦某个节点挂了，该节点对应的slot就无法使用，那么就会导致集群无法正常工作。 综上所述，每个Redis集群理论上最多可以有16384个节点。 Redis集群的搭建 Linux版本：CentOS7.6 x64Redis版本：6.0.9 所需环境Redis集群至少需要3个节点，因为投票容错机制要求超过半数节点认为某个节点挂了该节点才是挂了，所以2个节点无法构成集群。要保证集群的高可用，需要每个节点都有从节点，也就是备份节点，所以Redis集群至少需要6台服务器。这里搭建的是伪分布式集群，即一台服务器虚拟运行6个redis实例，修改端口号为（7001-7006）。 搭建步骤1.在/usr/local下创建redis-cluster目录，用来存放集群节点 2.将redis安装目录中bin目录下面的所有文件拷贝一份到/usr/local/redis-cluster/redis1目录下 1cp -r &#x2F;usr&#x2F;local&#x2F;redis&#x2F;bin&#x2F; &#x2F;usr&#x2F;local&#x2F;redis-cluster&#x2F;redis1 3.删除redis1目录下的快照文件dump.rdb，并且修改该目录下的redis.conf文件，具体修改两处地方：一是端口号修改为7001: 二是开启集群创建模式,cluster-enabled yes 的注释打开: 4.将redis1下的文件复制五份到redis2-redis6中 12345cp -r &#x2F;usr&#x2F;local&#x2F;redis-cluster&#x2F;redis1&#x2F; &#x2F;usr&#x2F;local&#x2F;redis-cluster&#x2F;redis2cp -r &#x2F;usr&#x2F;local&#x2F;redis-cluster&#x2F;redis1&#x2F; &#x2F;usr&#x2F;local&#x2F;redis-cluster&#x2F;redis3cp -r &#x2F;usr&#x2F;local&#x2F;redis-cluster&#x2F;redis1&#x2F; &#x2F;usr&#x2F;local&#x2F;redis-cluster&#x2F;redis4cp -r &#x2F;usr&#x2F;local&#x2F;redis-cluster&#x2F;redis1&#x2F; &#x2F;usr&#x2F;local&#x2F;redis-cluster&#x2F;redis5cp -r &#x2F;usr&#x2F;local&#x2F;redis-cluster&#x2F;redis1&#x2F; &#x2F;usr&#x2F;local&#x2F;redis-cluster&#x2F;redis6 创建完后，如图所示: 5.分别修改redis2-redis6下的redis.conf文件端口号为7002-7006 6.启动所有的redis节点，这里创建一个shell脚本 touch start-all.sh ，用于批量启动所有的redis节点，文件内容如下： 123456789101112131415161718cd redis1.&#x2F;redis-server redis.confcd ..cd redis2.&#x2F;redis-server redis.confcd ..cd redis3.&#x2F;redis-server redis.confcd ..cd redis4.&#x2F;redis-server redis.confcd ..cd redis5.&#x2F;redis-server redis.confcd ..cd redis6.&#x2F;redis-server redis.confcd .. 创建好启动脚本文件之后，需要修改该脚本的权限，使之能够执行，指令如下： 1chmod +x start-all.sh 执行shell脚本： 1.&#x2F;start-all.sh 6个redis节点是否启动成功，可以使用 ps aux|grep redis 命令查看： 7.执行集群相关命令，指定redis集群节点的ip和端口号，进入到任意一个redis的bin目录下，都可执行。 1.&#x2F;redis-cli --cluster create 127.0.0.1:7001 127.0.0.1:7002 127.0.0.1:7003 127.0.0.1:7004 127.0.0.1:7005 127.0.0.1:7006 --cluster-replicas 1 ps:上面命令中的1代表主节点和从节点的比值是多少，我们是3主3从，所以这个比值是1，而且前三个一定是主节点，redis就是这样规定的。 执行效果如图： 至此，Redi集群搭建成功！上面这个图显示了每个节点所分配的slots（哈希槽），这里总共6个节点，其中3个是从节点，所以3个主节点分别映射了0-5460、5461-10922、10933-16383solts。 最后连接集群节点，连接任意一个即可： 1.&#x2F;redis-cli -p 7001 -c ps:一定要加上-c，不然节点之间是无法自动跳转的！如下图可以看到，存储的数据（key-value）是均匀分配到不同的节点的：","categories":[{"name":"Redis","slug":"Redis","permalink":"https://zmq-git.github.io/categories/Redis/"}],"tags":[{"name":"教程","slug":"教程","permalink":"https://zmq-git.github.io/tags/%E6%95%99%E7%A8%8B/"}]},{"title":"linux搭建Redis教程","slug":"linux系统搭建Redis","date":"2021-03-19T13:59:19.591Z","updated":"2021-03-31T07:51:15.412Z","comments":true,"path":"2021/03/19/linux系统搭建Redis/","link":"","permalink":"https://zmq-git.github.io/2021/03/19/linux%E7%B3%BB%E7%BB%9F%E6%90%AD%E5%BB%BARedis/","excerpt":"","text":"Redis简介​ redis是一个基于key-value形式进行存储的内存型数据库 ​ 优点：效率高 理论值：每秒10k数据读取 ​ Redis是一个NoSql数据库,将其作为缓存工具使用（把某些使用频率较高的内容存储到Redis中) Redis持久化策略rdb​ 默认的持久化策略；每隔一段时间后把内存中数据持久化到dump.rdb文件中 ​ 缺点：数据过于集中,可能导致最后的数据没有持久化到dump.rdb中（可以使用save或BGSAVE命令手动持久化） aof​ 监听Redis的日志文件，监听如果发生了修改，删除，新增命令，立即根据这条命令把数据持久化 ​ 缺点：效率降低 Redis数据类型​ String 、Hash、 List 、 Set、 SortedSet Redis安装 Linux版本：CentOS7.6 x64Redis版本：6.0.9 安装gcc12345678910111213141516# 查看gcc版本是否在9.3以上，centos7.6默认安装4.8.5gcc -v# 升级gcc到9.3及以上,如下：升级到gcc 9.3：yum -y install centos-release-sclyum -y install devtoolset-9-gcc devtoolset-9-gcc-c++ devtoolset-9-binutilsscl enable devtoolset-9 bash需要注意的是scl命令启用只是临时的，退出shell或重启就会恢复原系统gcc版本。如果要长期使用gcc 9.3的话：echo &quot;source &#x2F;opt&#x2F;rh&#x2F;devtoolset-9&#x2F;enable&quot; &gt;&gt;&#x2F;etc&#x2F;profile这样退出shell重新打开就是新版的gcc了以下其他版本同理，修改devtoolset版本号即可。 解压编译将下载的压缩包上传到/usr/local目录下，解压压缩包 1tar zxf redis-6.0.9.tar.gz 进入解压后的目录进行编译 12cd redis-6.0.9make 指定目录安装 1make install PREFIX&#x3D;&#x2F;usr&#x2F;local&#x2F;redis 至此，Redis安装完毕。 启动Redis前台启动12cd &#x2F;usr&#x2F;local&#x2F;redis&#x2F;bin&#x2F;.&#x2F;redis-server ps: 这种启动方式不能关闭控制台，否则Redis就直接退出了 后台启动从 redis 的解压目录中复制 redis.conf 到 redis 的安装目录 1cp &#x2F;usr&#x2F;local&#x2F;redis-6.0.9&#x2F;redis.conf &#x2F;usr&#x2F;local&#x2F;redis&#x2F;bin&#x2F; 修改 redis.conf 文件，把 daemonize no 改为 daemonize yes ps: daemonize是用来指定redis是否要用守护线程的方式启动。 123456##启动命令：.&#x2F;redis-server redis.conf##停止命令.&#x2F;redis-cli shutdown##查看服务是否启动成功ps aux |grep redis 远程客户端连接Redis修改redis安装目录下的redis.conf文件，将bind:127.0.0.1注掉（使所有的ip都能访问到redis server） 打开防火墙：修改 /etc/sysconfig下面的iptables文件，添加 1-A INPUT -p tcp -m state --state NEW -m tcp --dport 6379 -j ACCEPT 最后运行 systemctl restart iptables 命令重启即可。 Redis密码设置如果需要设置Redis的密码，编辑redis.conf文件，找到requirepass，将注释放开并且更改密码。 1234###连接：.&#x2F;redis-cli -a 你的密码###停止：.&#x2F;redis-cli -a 你的密码 shutdown","categories":[{"name":"Redis","slug":"Redis","permalink":"https://zmq-git.github.io/categories/Redis/"}],"tags":[{"name":"教程","slug":"教程","permalink":"https://zmq-git.github.io/tags/%E6%95%99%E7%A8%8B/"}]}],"categories":[{"name":"JVM","slug":"JVM","permalink":"https://zmq-git.github.io/categories/JVM/"},{"name":"Mysql","slug":"Mysql","permalink":"https://zmq-git.github.io/categories/Mysql/"},{"name":"mybatis","slug":"mybatis","permalink":"https://zmq-git.github.io/categories/mybatis/"},{"name":"Docker","slug":"Docker","permalink":"https://zmq-git.github.io/categories/Docker/"},{"name":"FastDFS","slug":"FastDFS","permalink":"https://zmq-git.github.io/categories/FastDFS/"},{"name":"Java","slug":"Java","permalink":"https://zmq-git.github.io/categories/Java/"},{"name":"SpringBoot","slug":"SpringBoot","permalink":"https://zmq-git.github.io/categories/SpringBoot/"},{"name":"Web","slug":"Web","permalink":"https://zmq-git.github.io/categories/Web/"},{"name":"Redis","slug":"Redis","permalink":"https://zmq-git.github.io/categories/Redis/"}],"tags":[{"name":"学习笔记","slug":"学习笔记","permalink":"https://zmq-git.github.io/tags/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"},{"name":"教程","slug":"教程","permalink":"https://zmq-git.github.io/tags/%E6%95%99%E7%A8%8B/"},{"name":"Code","slug":"Code","permalink":"https://zmq-git.github.io/tags/Code/"}]}